---
title: 10x Your RAG Evaluation
subtitle: by Avoiding these Pitfalls
author: Skylar Payne
format: 
  revealjs:
    theme: [beige, styles.scss]
    smaller: true
    highlight-style: arrow
    table-format:
        hover: true
execute:
    enabled: false
---

## The Problem

RAG systems fail silently, and teams struggle to debug them.

:::{.columns}

:::{.column width="50%"}
![](assets/before_after.png)
:::

:::{.column width="50%"}

- "answer looks wrong" provides no actionable debugging path
- Silent failures accumulate as technical debt  
- Teams waste time guessing what's broken
:::

:::

**Goal**: Build systematic evaluation into your RAG pipeline

---

## About the Presenter

::: {.columns}

::: {.column width="30%"}
![Skylar Payne](https://skylarbpayne.com/assets/images/headshot.jpg)
:::

::: {.column width="70%"}
**Skylar Payne**  

> I'm not a career consultant. I'm a _builder_.

üòµ‚Äçüí´ AI is hard. So many questions, options, tradeoffs. I can help.

- **Decade of experience** building AI products at Google, LinkedIn, and in AI based diagnostics (healthcare)
- **Specializes in**: helping product & engineering teams to build AI products via evaluation-driven development
- **Philosophy**: Make it simple. Make it happen. Make it fun.

Fun fact: my favorite movie is Mean Girls (I have a party for it every October 3rd).
:::

:::

---

## The Only Thing to Remember

:::{.fragment}
**Look. At. Your. Data.**
:::

:::{.fragment}
Seriously. That's it.
:::

---

## Pitfall #1: Corpus Coverage

:::{.columns}

:::{.special style="margin-bottom: 2em;"}
Do we have all the documents we need?
:::

:::{.column width="60%"}
```
Use case: "How do I integrate with Salesforce?"
Search results: 0 relevant chunks

Reality: Integration exists but no docs were written

The cost: Lost customers who assume you don't support their stack
```

<!--TODO: the evaluation test should evaluate the % of queries which have relevant chunks across the entire corpus-->
**Evaluation Test**: Create "must answer" questions for your use cases - verify corpus coverage

:::

:::{.column width="10%"}
:::

:::{.column width="30%"}
![](assets/corpus_coverage.png)
:::
:::

---

## Pitfall #2: Information Extraction

:::{.special}
Do we understand the content well enough?
:::


```
Bad format:
"See attached PDF for API documentation (link broken)"

Good format:
"API Authentication: Use Bearer tokens in the Authorization header.
Example: Authorization: Bearer your_token_here"
```

<!--TODO: the evaluation test here should be to evaluate information extraction; eg are tables, etc extracted correctly?-->
**Evaluation Test**: Audit 20 docs - can key information be extracted without external dependencies?

![](assets/information_extraction.png)

---

## Pitfall #3: Chunk Quality

:::{.special}
Meaningful chunking that balances relevancy?
:::


```
Bad chunking:
Chunk 1: "To set up authentication, first install the SDK"
Chunk 2: "Then configure your API key in the dashboard"
Chunk 3: "Finally, add the auth middleware to your app"

Good chunking:
Chunk 1: "Authentication Setup:
1. Install SDK: npm install frigade-sdk
2. Configure API key in dashboard  
3. Add auth middleware to your app"
```

<!--TODO: test should be to look at a sample of large chunks, small chunks, chunks retrieved very often, chunks retrieved rarely, etc-->
**Evaluation Test**: Sample 50 chunks - can each standalone answer a complete question?

![](assets/chunk_quality.png)

---

## Pitfall #4: Query Rejection

:::{.special}
Was the query well-formed?
:::

```
Unclear query: "thing broken"
Clear query: "Why is my API authentication failing with 401 error?"

Unclear query: "pricing info"  
Clear query: "What are the pricing tiers for enterprise customers?"
```

<!--TODO: test should be rejection rate or elicitation rate of unclear queries-->
**Evaluation Test**: Sample 20 user queries - could a human understand the intent?

![](assets/vague_query.png)

---

## Pitfall #5: Retrieval Sufficiency

:::{.special}
Did we get enough relevant docs?
:::


```
Query: "How do I implement user segmentation?"
Retrieved chunks:
- "User segmentation allows personalized experiences" ‚úì relevant
- "Create segments based on user behavior" ‚úì relevant  
- "Segments can be used for targeting" ‚úì relevant

Missing: The actual implementation steps!
```

<!--TODO: test should be to evaluate the % of queries which have sufficient information retrieved-->
**Evaluation Test**: Give only retrieved chunks to humans - "Can you complete the task from this info?"

![](assets/sufficiency.png)

---

## Pitfall #6: Hallucination

:::{.special}
Did the generation hallucinate?
:::

```
Retrieved Context: "Our API rate limit is 1,000 requests per minute"
Generated Answer: "The API supports up to 500 requests per minute 
with burst capacity up to 2,000 for enterprise users"

Problem: Where did "500" and "enterprise burst capacity" come from?
```

<!--TODO: test should be to force in text citations and to validate them-->
**Evaluation Test**: Extract all claims from generated answers, verify each exists in retrieved context

![](assets/hallucination.png)

---

## A Generic Failure Funnel for RAG Systems

This is a generic failure funnel I like to use when evaluating RAG systems.  
Domain specific is always better, but I have found this to be a decent starting point.

```{mermaid}
flowchart LR
  A[Query Understood]
  B[Relevant Retrieval]
  C[Sufficient Retrieval]
  D[No Hallucination]
  E[Correct Answer]
  F[Right Format]

  A --> B
  B --> C
  C --> D
  D --> E
  E --> F
```

- queries that are vague or ambiguous need to elicit more information
- queries without any relevant retrieval likely mean information extraction is broken
- queries that do not have sufficient retrieval likely mean retrieval needs work
- queries that have hallucination likely mean context/prompting needs work
- queries that have incorrect answers may mean stronger models, reasoning, etc needed
- queries that have the wrong tone/format may require more examples, better prompting, etc

---

## Questions?

**Thank you!**

:::{.columns}

:::{.column width="50%"}
:::

:::{.column width="50%"}
:::{.special}
**Sign up for my newsletter!**
:::

![](https://api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://subscribe.skylarbpayne.com)
:::

:::
